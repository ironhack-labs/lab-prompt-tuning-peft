{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCA0sm55VaS-",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Lab | Introduction to Prompt Tuning using PEFT from Hugging Face\n",
        "\n",
        "<!-- ### Fine-tune a Foundational Model effortless -->\n",
        "\n",
        "**Note:** This is more or less the same notebook you saw in the previous lesson, but that is ok. This is an LLM fine-tuning lab. In class we used a set of datasets and models, and in the labs you are required to change the LLMs models and the datasets including the pre-processing pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6fba2d42-ed99-4a03-8033-d479ce24d5dd",
          "showTitle": false,
          "title": ""
        },
        "id": "2vkOvTEsVaTA"
      },
      "source": [
        "# Prompt Tuning\n",
        "\n",
        "## Brief introduction to Prompt Tuning.\n",
        "It’s an Additive Fine-Tuning technique for models. This means that we WILL NOT MODIFY ANY WEIGHTS OF THE ORIGINAL MODEL. You might be wondering, how are we going to perform fine-tuning then? Well, we will train additional layers that are added to the model. That’s why it’s called an Additive technique.\n",
        "\n",
        "Considering it’s an Additive technique and its name is Prompt-Tuning, it seems clear that the layers we’re going to add and train are related to the prompt.\n",
        "\n",
        "![My Image](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Martra_Figure_5_Prompt_Tuning.jpg?raw=true)\n",
        "\n",
        "We are creating a type of superprompt by enabling a model to enhance a portion of the prompt with its acquired knowledge. However, that particular section of the prompt cannot be translated into natural language. **It's as if we've mastered expressing ourselves in embeddings and generating highly effective prompts.**\n",
        "\n",
        "In each training cycle, the only weights that can be modified to minimize the loss function are those integrated into the prompt.\n",
        "\n",
        "The primary consequence of this technique is that the number of parameters to train is genuinely small. However, we encounter a second, perhaps more significant consequence, namely that, **since we do not modify the weights of the pretrained model, it does not alter its behavior or forget any information it has previously learned.**\n",
        "\n",
        "The training is faster and more cost-effective. Moreover, we can train various models, and during inference time, we only need to load one foundational model along with the new smaller trained models because the weights of the original model have not been altered\n",
        "\n",
        "## What are we going to do in the notebook?\n",
        "We are going to train two different models using two datasets, each with just one pre-trained model from the Bloom family. One will be trained to generate prompts and the other to detect hate in sentences.\n",
        "\n",
        "Additionally, we'll explore how to load both models with only one copy of the foundational model in memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZhdbTh-VaTA"
      },
      "source": [
        "## Loading the Peft Library\n",
        "This library contains the Hugging Face implementation of various fine-tuning techniques, including Prompt Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d16bf5ec-888b-4c76-a655-193fd4cc8a36",
          "showTitle": false,
          "title": ""
        },
        "id": "JechhJhhVaTA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n!pip install -q peft==0.10.0\\n!pip install -q datasets==2.18.0\\n!pip install -q transformers==4.39.3\\n!pip install -q accelerate==0.29.2\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "!pip install -q peft==0.10.0\n",
        "!pip install -q datasets==2.18.0\n",
        "!pip install -q transformers==4.39.3\n",
        "!pip install -q accelerate==0.29.2\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGbh426RVaTB"
      },
      "source": [
        "From the transformers library, we import the necessary classes to instantiate the model and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "5e05b6f6087c4e91b65d1d922e32de05",
            "faf2a83f634a40b6a9355381f08e5a11",
            "7aba321e4a4f4d92bdd85147c33384f1",
            "95fc397687f24181a78d1ec05197a659",
            "4cca2c8cd905497eb2f61f2a2688522e",
            "58e84a7dd52940c7a87623dcfc6e2570",
            "858dd935d1f1497abefa3eabdfaeab3a",
            "e026a2f814384f8f97ea0af8bb08336c",
            "80f125fab1d741a48145da4b27526938",
            "c444e5edf88e4921a02c87dd07cf8899",
            "dc6a99c262c24f3b83397ceb0dedcef8"
          ]
        },
        "id": "KWOEt-yOVaTB",
        "outputId": "e1f4ad4d-8df3-4615-dd91-1c1635b54798"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qYsnwjSVaTC"
      },
      "source": [
        "## Loading the model and the tokenizers.\n",
        "\n",
        "Bloom is one of the smallest and smartest models available for training with the PEFT Library using Prompt Tuning.\n",
        "\n",
        "I'm opting for the smallest one to minimize training time and avoid memory issues in Colab. Feel Free to try with a bigger one if you have acces to a good GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MnqIhv2UVaTC"
      },
      "outputs": [],
      "source": [
        "model_name = \"bigscience/bloom-560m\"\n",
        "NUM_VIRTUAL_TOKENS = 4\n",
        "#If you just want to test the solution, you can reduce the EPOCHs.\n",
        "NUM_EPOCHS_PROMPT = 3\n",
        "NUM_EPOCHS_CLASSIFIER = 2\n",
        "# device = \"auto\"\n",
        "device = \"mps\" #Replace with \"mps\" for Silicon chips."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSMu3qRsVaTC",
        "outputId": "da5b9c3c-5760-4b3f-cbf9-23a15c92be61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "W0926 01:40:46.254000 11401 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    #dtype=\"\"\n",
        "    #device_map = device\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W2fWhOnVaTC"
      },
      "source": [
        "## Inference with the pre trained bloom model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "47j2D3WWVaTC"
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100): #PLAY WITH THIS FUNCTION AS YOU SEE FIT\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        #temperature=0.2,\n",
        "        #top_p=0.95,\n",
        "        #do_sample=True,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=True, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ca4d203a-5152-4947-ab34-cfd0b40a102a",
          "showTitle": false,
          "title": ""
        },
        "id": "kRLSfuo2VaTC"
      },
      "source": [
        "To compare the pre-trained model with the same model after the prompt-tuning process, I will run the same sentence on both models.\n",
        "\n",
        "Since I'm creating a model that can generate prompts, I'll instruct it to provide a prompt that makes it act like a fitness trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "1d4c80a9-4edd-4fcd-aef0-996f4da5cc02",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvStaT7cVaTC",
        "outputId": "7b49bc65-359e-49b0-f1c7-ed45d5c526a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Act like a fitness trainer ive been doing this for about 3 months now and i have noticed that my body is getting better. I am also able to do more exercises than before, which makes me feel much healthier.\\nI was wondering if anyone else has had any success with']\n"
          ]
        }
      ],
      "source": [
        "input_prompt = tokenizer(\"Act like a fitness trainer \", return_tensors=\"pt\")\n",
        "foundational_outputs_prompt = get_outputs(foundational_model,\n",
        "                                          input_prompt.to(device),\n",
        "                                          max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-PY1yAh2mB"
      },
      "source": [
        "The model doesn't know what its mission is and answers as best as it can. It's not a bad response, but it's not what we're looking for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f438d43b-6b9f-445e-9df4-60ea09640764",
          "showTitle": false,
          "title": ""
        },
        "id": "OGbJTbRnVaTD"
      },
      "source": [
        "# Prompt Creator\n",
        "## Preparing Datasets\n",
        "The Dataset used, for this first example, is:\n",
        "* https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RD8H_LLaVaTD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "2ed62b41-e3fa-4a41-a0a9-59f35a6904f9",
          "showTitle": false,
          "title": ""
        },
        "id": "xmAp_o4PVaTD"
      },
      "outputs": [],
      "source": [
        "dataset_prompt = \"fka/awesome-chatgpt-prompts\"  # HF dataset id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qaU4FmgwAzZK"
      },
      "outputs": [],
      "source": [
        "def concatenate_columns_prompt(dataset):\n",
        "    def concatenate(example):\n",
        "        example['prompt'] = \"Act as a {}. Prompt: {}\".format(example['act'], example['prompt'])\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(concatenate)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e98454d191654df892344c5121196210",
            "e683fef138a04ba09489eec79fc93362",
            "9e48be389f3f494182ec13b7778c656a",
            "0e300d5917ca47f6b0f51f96cb98f8ec",
            "15aef94f2a4640c59e1f0fad3e75d206",
            "c5057f3071b443cead701f4e44a7da43",
            "7040d05f895243808b9aba24110a5655",
            "bc2d64590911437e8f0c37a63486b2ef",
            "49a1fedaa5c94cca9d86b4cf5fa2625d",
            "00f0cac8db414c56891deef759834c54",
            "dfb0661d4f7a4d439f2d00f0e825fb6e"
          ]
        },
        "id": "uoL6qitsLo0o",
        "outputId": "dac172da-c282-4fae-ceb8-1aa34b420471"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "#Create the Dataset to create prompts.\n",
        "data_prompt = load_dataset(dataset_prompt)\n",
        "data_prompt['train'] = concatenate_columns_prompt(data_prompt['train'])\n",
        "\n",
        "# Tokenization with correct parameters\n",
        "data_prompt = data_prompt.map(\n",
        "    lambda samples: tokenizer(\n",
        "        samples[\"prompt\"], \n",
        "        padding=True, \n",
        "        truncation=True, \n",
        "        max_length=512,\n",
        "        return_tensors=None  # Don't return tensors at this stage\n",
        "    ), \n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# Remove all text columns, keep only tokenized data\n",
        "train_sample_prompt = data_prompt[\"train\"].remove_columns(['act', 'prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL1CwC9tGGSn",
        "outputId": "3b7e4dcd-1f23-4ed5-a2e7-91cedfcd03c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset structure for training:\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 203\n",
            "})\n",
            "\n",
            "Data example:\n",
            "{'input_ids': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8972, 661, 267, 2246, 28857, 167625, 170786, 17, 36949, 1309, 29, 156301, 1152, 1306, 660, 72560, 28857, 167625, 84544, 20165, 376, 1002, 26168, 267, 30479, 17477, 613, 267, 120755, 238776, 17, 1387, 47881, 632, 427, 14565, 29866, 664, 368, 120755, 15, 16997, 4054, 136044, 375, 4859, 12, 427, 39839, 15, 9697, 1242, 375, 13614, 12, 3804, 427, 368, 2298, 5268, 109891, 368, 17477, 15, 530, 427, 11210, 4143, 7112, 11866, 368, 11011, 1620, 36320, 17, 21265, 267, 11550, 90533, 30479, 17477, 613, 1119, 27343, 15, 11762, 368, 18348, 16231, 530, 127246, 613, 94510, 368, 25605, 55790, 17, 29901, 13842, 368, 4400, 530, 2914, 24466, 184637, 427, 22646, 267, 11285, 32391, 461, 368, 17786, 17], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset structure for training:\")\n",
        "print(train_sample_prompt)\n",
        "print(\"\\nData example:\")\n",
        "print(train_sample_prompt[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzA3rLTk8XW8",
        "outputId": "8dd14e6c-d90b-4730-a91f-5e447d0e4911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8972, 661, 267, 2246, 28857, 167625, 170786, 17, 36949, 1309, 29, 156301, 1152, 1306, 660, 72560, 28857, 167625, 84544, 20165, 376, 1002, 26168, 267, 30479, 17477, 613, 267, 120755, 238776, 17, 1387, 47881, 632, 427, 14565, 29866, 664, 368, 120755, 15, 16997, 4054, 136044, 375, 4859, 12, 427, 39839, 15, 9697, 1242, 375, 13614, 12, 3804, 427, 368, 2298, 5268, 109891, 368, 17477, 15, 530, 427, 11210, 4143, 7112, 11866, 368, 11011, 1620, 36320, 17, 21265, 267, 11550, 90533, 30479, 17477, 613, 1119, 27343, 15, 11762, 368, 18348, 16231, 530, 127246, 613, 94510, 368, 25605, 55790, 17, 29901, 13842, 368, 4400, 530, 2914, 24466, 184637, 427, 22646, 267, 11285, 32391, 461, 368, 17786, 17], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8972, 661, 267, 174249, 36949, 1309, 17, 36949, 1309, 29, 43252, 15202, 51, 46712, 15, 7932, 660, 67606, 613, 660, 8723, 861, 2152, 722, 415, 15, 1874, 17848, 664, 368, 70062, 38038, 388, 174249, 39841, 9427, 11173, 664, 368, 7921, 1581, 9649, 1485, 8943, 17, 137151, 6216, 24466, 87480, 6399, 17, 109988, 368, 70062, 32993, 461, 368, 10082, 3164, 6426, 17, 5070, 5546, 13773, 461, 368, 67606, 15, 13756, 368, 14679, 11210, 17, 137151, 209147, 86, 13773, 361, 368, 67606, 10136, 15, 11173, 664, 6199, 3466, 9283, 13773, 1485, 8943, 613, 368, 70062, 17, 3904, 67606, 6591, 722, 5636, 53180, 530, 65604, 15, 1427, 861, 473, 1400, 7932, 267, 415, 15, 1874, 14679, 8723, 1485, 718, 17, 143293, 267, 3829, 4737, 461, 499, 49863, 530, 557, 103096, 135158, 17251, 427, 2670, 70062, 17, 30497, 13756, 2914, 3390, 17848, 17251, 427, 368, 70062, 17, 121045, 1074, 267, 4737, 461, 735, 24466, 26331, 31437, 427, 13756, 530, 368, 49635, 91770, 5484, 17, 42187, 11097, 3291, 87099, 1130, 174169, 14476, 17, 152830, 368, 67606, 3727, 1571, 404, 530, 1571, 415, 17]], 'attention_mask': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample_prompt[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b97381d4-5fe2-49d0-be5d-2fe3421edc5c",
          "showTitle": false,
          "title": ""
        },
        "id": "0-5mv1ZpVaTD"
      },
      "source": [
        "## prompt-tuning configuration.  \n",
        "\n",
        "API docs:\n",
        "https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6df8e1f1-be9e-42db-b4a4-6af7cd351004",
          "showTitle": false,
          "title": ""
        },
        "id": "sOg1Yh-oVaTD"
      },
      "outputs": [],
      "source": [
        "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
        "\n",
        "generation_config_prompt = PromptTuningConfig( #PLAY WITH THIS CONFIG IF YOU LIKE\n",
        "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
        "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an9KBtB1VaTD"
      },
      "source": [
        "We will create two  prompt tuning models using the same pre-trained model and the same config, but with a different Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_D8oDQZVaTD",
        "outputId": "44cbd409-873e-4f92-88b4-bb6698210082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "peft_model_prompt = get_peft_model(foundational_model, generation_config_prompt)\n",
        "print(peft_model_prompt.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cff5bc33-8cfb-4144-8962-9c54362a7faa",
          "showTitle": false,
          "title": ""
        },
        "id": "i6WhJSUwVaTE"
      },
      "source": [
        "**That's amazing: did you see the reduction in trainable parameters? We are going to train a 0.001% of the paramaters available.**\n",
        "\n",
        "Now we are going to create the training arguments, and we will use the same configuration in both trainings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SJoznfzjVaTE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "def create_training_arguments(path, learning_rate=0.0035, epochs=6, autobatch=True, use_cpu=False):\n",
        "    # Determine device\n",
        "    if use_cpu or not torch.backends.mps.is_available():\n",
        "        device_args = {\n",
        "            \"use_cpu\": True,\n",
        "            \"per_device_train_batch_size\": 2,\n",
        "            \"gradient_accumulation_steps\": 2,\n",
        "            \"dataloader_pin_memory\": False,\n",
        "        }\n",
        "    else:\n",
        "        # MPS settings with memory constraints\n",
        "        device_args = {\n",
        "            \"use_cpu\": False,\n",
        "            \"per_device_train_batch_size\": 1,\n",
        "            \"gradient_accumulation_steps\": 4,\n",
        "            \"dataloader_pin_memory\": False,\n",
        "        }\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
        "        auto_find_batch_size=autobatch, # Find a suitable batch size that will fit into memory automatically\n",
        "        learning_rate= learning_rate, # Higher learning rate than full fine-tuning\n",
        "        num_train_epochs=epochs,\n",
        "        save_strategy=\"epoch\", # Save after each epoch\n",
        "        logging_steps=10, # Logging every 10 steps\n",
        "        save_total_limit=2, # Limit number of saved checkpoints\n",
        "        remove_unused_columns=False, # Keep all columns\n",
        "        **device_args\n",
        "    )\n",
        "    return training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "54b78a8f-81f0-44c0-b0bc-dcb14891715f",
          "showTitle": false,
          "title": ""
        },
        "id": "cb1j50DSVaTE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "working_dir = \"./\"\n",
        "\n",
        "#Is best to store the models in separate folders.\n",
        "#Create the name of the directories where to store the models.\n",
        "output_directory_prompt =  os.path.join(working_dir, \"peft_outputs_prompt\")\n",
        "output_directory_classifier =  os.path.join(working_dir, \"peft_outputs_classifier\")\n",
        "\n",
        "#Just creating the directoris if not exist.\n",
        "if not os.path.exists(working_dir):\n",
        "    os.mkdir(working_dir)\n",
        "if not os.path.exists(output_directory_prompt):\n",
        "    os.mkdir(output_directory_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC5IhO9mVaTE"
      },
      "source": [
        "We need to indicate the directory containing the model when creating the TrainingArguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c593deb6-5626-4fd9-89c2-2329e2f9b6e0",
          "showTitle": false,
          "title": ""
        },
        "id": "GdMfjk5RVaTE"
      },
      "source": [
        "## Training first model\n",
        "\n",
        "We will create the trainer Object, one for each model to train.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "D4v4RSSeVaTE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NUM_EPOCHS_PROMPT: 3\n",
            "output_directory_prompt: ./peft_outputs_prompt\n",
            "Training arguments created successfully!\n",
            "Batch size: 1\n",
            "Gradient accumulation steps: 4\n",
            "Use CPU: False\n"
          ]
        }
      ],
      "source": [
        "# Check variables\n",
        "print(f\"NUM_EPOCHS_PROMPT: {NUM_EPOCHS_PROMPT}\")\n",
        "print(f\"output_directory_prompt: {output_directory_prompt}\")\n",
        "\n",
        "# Try MPS first\n",
        "training_args_prompt = create_training_arguments(output_directory_prompt,\n",
        "                                                 3e-2,\n",
        "                                                 NUM_EPOCHS_PROMPT,\n",
        "                                                 use_cpu=False)  # Try MPS first\n",
        "\n",
        "# Check created arguments\n",
        "print(\"Training arguments created successfully!\")\n",
        "print(f\"Batch size: {training_args_prompt.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation steps: {training_args_prompt.gradient_accumulation_steps}\")\n",
        "print(f\"Use CPU: {training_args_prompt.use_cpu}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uVAfNdEIVaTE"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling, TrainerCallback\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Custom callback for memory cleanup\n",
        "class MemoryCleanupCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if torch.backends.mps.is_available():\n",
        "            torch.mps.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "def create_trainer(model, training_args, train_dataset):\n",
        "    # Create data collator with correct settings\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, \n",
        "        mlm=False,  # mlm=False indicates not to use masked language modeling\n",
        "        pad_to_multiple_of=8,  # Alignment for efficiency\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
        "        args=training_args, #The args for the training.\n",
        "        train_dataset=train_dataset, #The dataset used to train the model.\n",
        "        data_collator=data_collator,\n",
        "        callbacks=[MemoryCleanupCallback()] # Add callback for memory cleanup\n",
        "    )\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "32e43bcf-23b2-46aa-9cf0-455b83ef4f38",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "1Sz9BeFZVaTF",
        "outputId": "8f9f1dfb-8477-4d42-d97d-1c07296418c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking dataset structure:\n",
            "Columns: ['input_ids', 'attention_mask']\n",
            "Dataset size: 203\n",
            "Sample record: {'input_ids': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8972, 661, 267, 2246, 28857, 167625, 170786, 17, 36949, 1309, 29, 156301, 1152, 1306, 660, 72560, 28857, 167625, 84544, 20165, 376, 1002, 26168, 267, 30479, 17477, 613, 267, 120755, 238776, 17, 1387, 47881, 632, 427, 14565, 29866, 664, 368, 120755, 15, 16997, 4054, 136044, 375, 4859, 12, 427, 39839, 15, 9697, 1242, 375, 13614, 12, 3804, 427, 368, 2298, 5268, 109891, 368, 17477, 15, 530, 427, 11210, 4143, 7112, 11866, 368, 11011, 1620, 36320, 17, 21265, 267, 11550, 90533, 30479, 17477, 613, 1119, 27343, 15, 11762, 368, 18348, 16231, 530, 127246, 613, 94510, 368, 25605, 55790, 17, 29901, 13842, 368, 4400, 530, 2914, 24466, 184637, 427, 22646, 267, 11285, 32391, 461, 368, 17786, 17], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Creating trainer...\n",
            "Clearing MPS memory...\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4327e4852ce241ae89eefb261cb296c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/150 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.5173, 'grad_norm': 0.02167588658630848, 'learning_rate': 0.028, 'epoch': 0.2}\n",
            "{'loss': 3.431, 'grad_norm': 0.01950989104807377, 'learning_rate': 0.026, 'epoch': 0.39}\n",
            "{'loss': 3.2785, 'grad_norm': 0.023076007142663002, 'learning_rate': 0.024, 'epoch': 0.59}\n",
            "{'loss': 3.1312, 'grad_norm': 0.03632348030805588, 'learning_rate': 0.022, 'epoch': 0.79}\n",
            "{'loss': 3.1195, 'grad_norm': 0.03798944130539894, 'learning_rate': 0.019999999999999997, 'epoch': 0.99}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 92da68f9-a65b-466b-9a40-61762191670b)') - silently ignoring the lookup for the file config.json in bigscience/bloom-560m.\n",
            "  warnings.warn(\n",
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in bigscience/bloom-560m - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9893, 'grad_norm': 0.02155931107699871, 'learning_rate': 0.018, 'epoch': 1.18}\n",
            "{'loss': 2.9413, 'grad_norm': 0.03059413470327854, 'learning_rate': 0.016, 'epoch': 1.38}\n",
            "{'loss': 2.9346, 'grad_norm': 0.0388522669672966, 'learning_rate': 0.014, 'epoch': 1.58}\n",
            "{'loss': 2.8975, 'grad_norm': 0.036616694182157516, 'learning_rate': 0.012, 'epoch': 1.77}\n",
            "{'loss': 2.9279, 'grad_norm': 0.03003859706223011, 'learning_rate': 0.009999999999999998, 'epoch': 1.97}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9454, 'grad_norm': 0.07816430181264877, 'learning_rate': 0.008, 'epoch': 2.17}\n",
            "{'loss': 2.8242, 'grad_norm': 0.03052625060081482, 'learning_rate': 0.006, 'epoch': 2.36}\n",
            "{'loss': 2.7884, 'grad_norm': 0.03508185222744942, 'learning_rate': 0.004, 'epoch': 2.56}\n",
            "{'loss': 2.7976, 'grad_norm': 0.04300406947731972, 'learning_rate': 0.002, 'epoch': 2.76}\n",
            "{'loss': 2.8173, 'grad_norm': 0.029569586738944054, 'learning_rate': 0.0, 'epoch': 2.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 1027.3559, 'train_samples_per_second': 0.593, 'train_steps_per_second': 0.146, 'train_loss': 3.022738431294759, 'epoch': 2.96}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=150, training_loss=3.022738431294759, metrics={'train_runtime': 1027.3559, 'train_samples_per_second': 0.593, 'train_steps_per_second': 0.146, 'train_loss': 3.022738431294759, 'epoch': 2.96})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set environment variables for MPS\n",
        "import os\n",
        "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
        "\n",
        "# Check dataset structure\n",
        "print(\"Checking dataset structure:\")\n",
        "print(f\"Columns: {train_sample_prompt.column_names}\")\n",
        "print(f\"Dataset size: {len(train_sample_prompt)}\")\n",
        "print(f\"Sample record: {train_sample_prompt[0]}\")\n",
        "\n",
        "#Training first model.\n",
        "print(\"Creating trainer...\")\n",
        "trainer_prompt = create_trainer(peft_model_prompt,\n",
        "                                training_args_prompt,\n",
        "                                train_sample_prompt)\n",
        "\n",
        "# Memory cleanup before training\n",
        "import torch\n",
        "import gc\n",
        "if torch.backends.mps.is_available():\n",
        "    torch.mps.empty_cache()\n",
        "    print(\"Clearing MPS memory...\")\n",
        "elif torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Clearing CUDA memory...\")\n",
        "gc.collect()\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer_prompt.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clearing MPS memory...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_mps_memory():\n",
        "    if torch.backends.mps.is_available():\n",
        "        torch.mps.empty_cache()\n",
        "        print(\"Clearing MPS memory...\")\n",
        "    elif torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"Clearing CUDA memory...\")\n",
        "    gc.collect()\n",
        "\n",
        "clear_mps_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Veg8ziHvWh4I"
      },
      "source": [
        "Release GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYVcJDSP7Sq-",
        "outputId": "cc870718-95e2-4921-c1dc-424773ba3883"
      },
      "outputs": [],
      "source": [
        "# Function for MPS memory cleanup\n",
        "def clear_mps_memory():\n",
        "    import torch\n",
        "    import gc\n",
        "    if torch.backends.mps.is_available():\n",
        "        torch.mps.empty_cache()\n",
        "    elif torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Memory cleanup\n",
        "clear_mps_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5a6c8daf-8248-458a-9f6f-14865b4fbd2e",
          "showTitle": false,
          "title": ""
        },
        "id": "s5k10HwoVaTG"
      },
      "source": [
        "## Save model\n",
        "We are going to save the model. These models are ready to be used, as long as we have the pre-trained model from which they were created in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "409df5ce-e496-46d7-be2c-202a463cdc80",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3dn3PeMVaTG",
        "outputId": "31699dd9-0f82-43e7-e708-6b5f2992085d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer_prompt.model.save_pretrained(output_directory_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "fb14e3fd-bbf6-4d56-92c2-51bfe08de72a",
          "showTitle": false,
          "title": ""
        },
        "id": "rkUKpDDWVaTG"
      },
      "source": [
        "## Inference first tuned model\n",
        "\n",
        "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cc48af16-c117-4019-a31a-ce1c93cd21d4",
          "showTitle": false,
          "title": ""
        },
        "id": "dlqXXN8oVaTG"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "loaded_model_peft = PeftModel.from_pretrained(foundational_model,\n",
        "                                         output_directory_prompt,\n",
        "                                         #device_map=device,\n",
        "                                         is_trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjXT-6EKMiXk",
        "outputId": "644d2cc2-156b-429c-a27b-b7ee4e68d155"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"Act like a fitness trainer  response: I will act as an expert in my field. My goal is to be the best at what i do and make it work for me, so that people can learn from us or find out more about our company's products & services by doing\"]\n"
          ]
        }
      ],
      "source": [
        "loaded_model_prompt_outputs = get_outputs(loaded_model_peft,\n",
        "                                          input_prompt,\n",
        "                                          max_new_tokens=50)\n",
        "print(tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzjDgDE2reTO"
      },
      "source": [
        "Let's compare the result of the model before and after being fine-tuned with prompt-tuning.\n",
        "\n",
        "**Input for the model**\n",
        "```\n",
        "Act as a fitness Trainer. Prompt:\n",
        "```\n",
        "\n",
        "**Original model**\n",
        "```\n",
        "Act as a fitness Trainer. Prompt:  Follow up with your trainer\n",
        "```\n",
        "**Trained for classification with Prompt-tuning** 50 Epochs:\n",
        "```\n",
        "Act as a fitness Trainer. Prompt: ＋ Acts like an expert in the field of sports and health, but does not provide detailed information about his work or products to help you understand them better.  + I want my first client referred me through this website for their gym membership program which is based on physical activity training exercises that are easy enough (eight minutes) per week with no need any special equipment required.   - First Question : What would be your role?\n",
        "```\n",
        "\n",
        "It's very clear that the result is quite different, it's not exactly what we're looking for but it's much closer.\n",
        "\n",
        "It's possible that we're at the limit of what Bloom's smallest model can offer. Try with any other model, surely with the one with 1B parameters the result will be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVif-42UCP7l"
      },
      "source": [
        "# Hate Classifier\n",
        "##Loading the Dataset\n",
        "\n",
        "* https://huggingface.co/datasets/SetFit/ethos_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyp64F0tRQBt",
        "outputId": "8b2f0c49-83df-4a17-a53c-c09655f2b229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"Classify whether the message is hateful or not:  if it was, then we would have to check for a negative score. If it's positive and we're still in doubt about its content (easier said than done), we'll just say that.\\nIf you want more information on how this works please read\"]\n"
          ]
        }
      ],
      "source": [
        "input_classifier = tokenizer(\"Classify whether the message is hateful or not: \", return_tensors=\"pt\")\n",
        "foundational_outputs_prompt = get_outputs(foundational_model,\n",
        "                                          input_classifier.to(device),\n",
        "                                          max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1gk7C_NSt-Y"
      },
      "source": [
        "The model has no idea what its purpose is, so it completes the sentence as best as it can."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wlLAsUvhE09L"
      },
      "outputs": [],
      "source": [
        "dataset_classifier = \"SetFit/ethos_binary\"\n",
        "\n",
        "def concatenate_columns_classifier(dataset):\n",
        "    def concatenate(example):\n",
        "        example['text'] = \"Sentence : {} Label : {}\".format(example['text'], example['label_text'])\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(concatenate)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y2K_3MqE4QV",
        "outputId": "48524b8b-afe6-42bc-91e7-493f2e0e2d5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "data_classifier = load_dataset(dataset_classifier)\n",
        "data_classifier['train'] = concatenate_columns_classifier(data_classifier['train'])\n",
        "\n",
        "data_classifier = data_classifier.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
        "train_sample_classifier = data_classifier[\"train\"].remove_columns(['label', 'label_text', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia-vz3ddTOY5",
        "outputId": "11f3eced-0083-46ad-99cf-ee2836c638f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 598\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 400\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8ROxvTEFAVC",
        "outputId": "6e5e079e-1f01-4110-d9ac-e22a15eda27f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 598\n",
              "})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_sample_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV2Z_LiRTMDC"
      },
      "source": [
        "I have deleted all the columns from the dataset that are not strictly necessary for training, that is to say, I have removed all columns that are not essential for the model's learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFUZNrAdFDR5",
        "outputId": "d07377f4-a9f3-40ae-d8b7-751312471894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[62121, 1671, 915, 473, 760, 10190, 513, 16154, 60, 19821, 138929, 20812, 426, 18833, 18816, 75536, 45617, 39469, 19368, 17956, 57274, 3758, 18065, 38, 44140, 17956, 72870, 8309, 9492, 15, 614, 156801, 85061, 48283, 44419, 426, 16472, 96789, 602, 45227, 43111, 181485, 435, 19821, 60, 48283, 44419, 426, 16472, 96789, 614, 156801, 77658, 915, 74549, 40423]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample_classifier[1:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_F3oR70FF4z"
      },
      "source": [
        "## prompt-tuning configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vr0Aw_byFL6n"
      },
      "outputs": [],
      "source": [
        "generation_config_classifier = PromptTuningConfig( #PLAY WITH THIS AS YOU SEE FIT\n",
        "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,  #\n",
        "    prompt_tuning_init_text=\"Indicates whether the sentence contains hate speech or not\",\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
        "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWvoJMajFPcg",
        "outputId": "e232bc59-3bf4-4f45-e70d-5ea4e5623b2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "peft_model_classifier = get_peft_model(foundational_model, generation_config_classifier)\n",
        "print(peft_model_classifier.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CpKtEudsFWTq"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(output_directory_classifier):\n",
        "    os.mkdir(output_directory_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DknsYEGwFW4g"
      },
      "outputs": [],
      "source": [
        "training_args_classifier = create_training_arguments(output_directory_classifier,\n",
        "                                                    3e-2,\n",
        "                                                    NUM_EPOCHS_CLASSIFIER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAgEAjAMFasw"
      },
      "source": [
        "## Training Second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "HfDLVy22FaNs",
        "outputId": "78ad3ad8-aaff-48e3-b15e-8d2e5d64d937"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82aa14c752a44ad08d6101bf1d6e4d2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/298 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 5.025, 'grad_norm': 0.40504950284957886, 'learning_rate': 0.028993288590604026, 'epoch': 0.07}\n",
            "{'loss': 3.9117, 'grad_norm': 0.43498456478118896, 'learning_rate': 0.02798657718120805, 'epoch': 0.13}\n",
            "{'loss': 3.7938, 'grad_norm': 0.31533288955688477, 'learning_rate': 0.02697986577181208, 'epoch': 0.2}\n",
            "{'loss': 3.5373, 'grad_norm': 0.25324952602386475, 'learning_rate': 0.025973154362416106, 'epoch': 0.27}\n",
            "{'loss': 3.4766, 'grad_norm': 0.2528357207775116, 'learning_rate': 0.024966442953020133, 'epoch': 0.33}\n",
            "{'loss': 3.3582, 'grad_norm': 0.15943555533885956, 'learning_rate': 0.023959731543624158, 'epoch': 0.4}\n",
            "{'loss': 3.4446, 'grad_norm': 0.24668918550014496, 'learning_rate': 0.02295302013422819, 'epoch': 0.47}\n",
            "{'loss': 3.1682, 'grad_norm': 0.33712857961654663, 'learning_rate': 0.021946308724832216, 'epoch': 0.54}\n",
            "{'loss': 3.4815, 'grad_norm': 0.18338169157505035, 'learning_rate': 0.02093959731543624, 'epoch': 0.6}\n",
            "{'loss': 3.4263, 'grad_norm': 0.2700563371181488, 'learning_rate': 0.019932885906040268, 'epoch': 0.67}\n",
            "{'loss': 3.2743, 'grad_norm': 0.1522781401872635, 'learning_rate': 0.018926174496644296, 'epoch': 0.74}\n",
            "{'loss': 3.2338, 'grad_norm': 0.20372067391872406, 'learning_rate': 0.017919463087248323, 'epoch': 0.8}\n",
            "{'loss': 3.4499, 'grad_norm': 0.1287866234779358, 'learning_rate': 0.016912751677852347, 'epoch': 0.87}\n",
            "{'loss': 3.141, 'grad_norm': 0.2766849100589752, 'learning_rate': 0.015906040268456375, 'epoch': 0.94}\n",
            "{'loss': 3.1657, 'grad_norm': 0.30819451808929443, 'learning_rate': 0.014899328859060403, 'epoch': 1.0}\n",
            "{'loss': 3.0053, 'grad_norm': 0.21902020275592804, 'learning_rate': 0.013892617449664428, 'epoch': 1.07}\n",
            "{'loss': 3.3027, 'grad_norm': 0.2133971005678177, 'learning_rate': 0.012885906040268456, 'epoch': 1.14}\n",
            "{'loss': 3.2888, 'grad_norm': 0.09871137887239456, 'learning_rate': 0.011879194630872482, 'epoch': 1.2}\n",
            "{'loss': 3.2187, 'grad_norm': 0.15138226747512817, 'learning_rate': 0.01087248322147651, 'epoch': 1.27}\n",
            "{'loss': 3.5034, 'grad_norm': 0.14778512716293335, 'learning_rate': 0.009865771812080537, 'epoch': 1.34}\n",
            "{'loss': 3.2643, 'grad_norm': 0.14559292793273926, 'learning_rate': 0.008859060402684565, 'epoch': 1.4}\n",
            "{'loss': 3.1202, 'grad_norm': 0.26394346356391907, 'learning_rate': 0.00785234899328859, 'epoch': 1.47}\n",
            "{'loss': 3.2475, 'grad_norm': 0.14790572226047516, 'learning_rate': 0.006845637583892617, 'epoch': 1.54}\n",
            "{'loss': 3.1785, 'grad_norm': 0.18660655617713928, 'learning_rate': 0.005838926174496644, 'epoch': 1.61}\n",
            "{'loss': 3.2343, 'grad_norm': 0.252467542886734, 'learning_rate': 0.004832214765100671, 'epoch': 1.67}\n",
            "{'loss': 3.2659, 'grad_norm': 0.21705199778079987, 'learning_rate': 0.003825503355704698, 'epoch': 1.74}\n",
            "{'loss': 3.2768, 'grad_norm': 0.2325076460838318, 'learning_rate': 0.0028187919463087247, 'epoch': 1.81}\n",
            "{'loss': 3.2547, 'grad_norm': 0.25145211815834045, 'learning_rate': 0.0018120805369127517, 'epoch': 1.87}\n",
            "{'loss': 3.244, 'grad_norm': 0.18915103375911713, 'learning_rate': 0.0008053691275167785, 'epoch': 1.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 301.7975, 'train_samples_per_second': 3.963, 'train_steps_per_second': 0.987, 'train_loss': 3.384606092568212, 'epoch': 1.99}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=298, training_loss=3.384606092568212, metrics={'train_runtime': 301.7975, 'train_samples_per_second': 3.963, 'train_steps_per_second': 0.987, 'train_loss': 3.384606092568212, 'epoch': 1.99})"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer_classifier = create_trainer(peft_model_classifier,\n",
        "                                   training_args_classifier,\n",
        "                                   train_sample_classifier)\n",
        "trainer_classifier.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcWL4-4OKCx9",
        "outputId": "2bf51352-7169-46a7-c505-275c1aa026f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sergej/python_projects/hfenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer_classifier.model.save_pretrained(output_directory_classifier, force_download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRcSaXMmM3mz"
      },
      "source": [
        "## Inference second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5Pkx0npHNWja"
      },
      "outputs": [],
      "source": [
        "loaded_model_peft.load_adapter(output_directory_classifier, adapter_name=\"classifier\")\n",
        "loaded_model_peft.set_adapter(\"classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an8T4LvPJAUC",
        "outputId": "de9a7821-8f76-46ed-998e-b5509e36f1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Testing hate speech classifier model ===\n",
            "Input: Head is the shape of a light bulb.\n",
            "Expected result: NOT hate speech\n",
            "Full result: Sentence : Head is the shape of a light bulb. Label : no hate speech Label : hate speech Label : hate\n",
            "Model prediction: hate\n",
            "============================================================\n",
            "Input: I don't like short people, no idea why they exist.\n",
            "Expected result: hate speech\n",
            "Full result: Sentence : I don't like short people, no idea why they exist. Label : no hate speech Label : hate speech Label : hate\n",
            "Model prediction: hate\n",
            "============================================================\n",
            "Input: The weather is nice today.\n",
            "Expected result: NOT hate speech\n",
            "Full result: Sentence : The weather is nice today. Label : no hate speech Label : hate speech Label : hate\n",
            "Model prediction: hate\n",
            "============================================================\n",
            "Input: I hate all people from that country.\n",
            "Expected result: hate speech\n",
            "Full result: Sentence : I hate all people from that country. Label : no hate speech Label : hate speech Label : hate\n",
            "Model prediction: hate\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Test hate speech classifier model\n",
        "print(\"=== Testing hate speech classifier model ===\")\n",
        "\n",
        "# Make sure torch is imported\n",
        "import torch\n",
        "\n",
        "# Test examples\n",
        "test_examples = [\n",
        "    (\"Head is the shape of a light bulb.\", \"NOT hate speech\"),\n",
        "    (\"I don't like short people, no idea why they exist.\", \"hate speech\"),\n",
        "    (\"The weather is nice today.\", \"NOT hate speech\"),\n",
        "    (\"I hate all people from that country.\", \"hate speech\")\n",
        "]\n",
        "\n",
        "for example, expected in test_examples:\n",
        "    # Format input text in the same format used during training\n",
        "    input_text = f\"Sentence : {example} Label :\"\n",
        "    input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    \n",
        "    # Get prediction from model\n",
        "    with torch.no_grad():\n",
        "        outputs = loaded_model_peft.generate(\n",
        "            input_ids=input_tokens[\"input_ids\"].to(device),\n",
        "            attention_mask=input_tokens[\"attention_mask\"].to(device),\n",
        "            max_new_tokens=10,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode result\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    prediction = result.split(\"Label :\")[-1].strip()\n",
        "    \n",
        "    print(f\"Input: {example}\")\n",
        "    print(f\"Expected result: {expected}\")\n",
        "    print(f\"Full result: {result}\")\n",
        "    print(f\"Model prediction: {prediction}\")\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHbeFTXjVaTG"
      },
      "source": [
        "Let's check how the model's response has changed with training:\n",
        "\n",
        "**Input for the model**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :\n",
        "Sentence : I don't liky short people, no idea why they exist. Label :\n",
        "```\n",
        "\n",
        "**Original model**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :  head\n",
        "Sentence : I don't liky short people, no idea why they exist. Label :  No\n",
        "```\n",
        "**Trained for classification with Prompt-tuning**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :  no hate speech\n",
        "Sentence : I don't liky short people, no idea why they exist. Label :  hate speech\n",
        "```\n",
        "\n",
        "It's clear that the training has fulfilled its purpose. The original model doesn't know what its mission is and tries to complete the sentences as best as it can. On the other hand, the updated model with prompt-tuning does know what its mission is and is able to classify the sentences correctly and in the indicated format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6TUjNtGVaTH"
      },
      "source": [
        "# Exercise\n",
        "- Complete the prompts similar to what we did in class.\n",
        "     - Try at least 3 versions\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "LLM 02 - Prompt Tuning with PEFT",
      "widgets": {}
    },
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "hfenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00f0cac8db414c56891deef759834c54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e300d5917ca47f6b0f51f96cb98f8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00f0cac8db414c56891deef759834c54",
            "placeholder": "​",
            "style": "IPY_MODEL_dfb0661d4f7a4d439f2d00f0e825fb6e",
            "value": " 203/203 [00:00&lt;00:00, 925.99 examples/s]"
          }
        },
        "15aef94f2a4640c59e1f0fad3e75d206": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a1fedaa5c94cca9d86b4cf5fa2625d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cca2c8cd905497eb2f61f2a2688522e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e84a7dd52940c7a87623dcfc6e2570": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e05b6f6087c4e91b65d1d922e32de05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_faf2a83f634a40b6a9355381f08e5a11",
              "IPY_MODEL_7aba321e4a4f4d92bdd85147c33384f1",
              "IPY_MODEL_95fc397687f24181a78d1ec05197a659"
            ],
            "layout": "IPY_MODEL_4cca2c8cd905497eb2f61f2a2688522e"
          }
        },
        "7040d05f895243808b9aba24110a5655": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7aba321e4a4f4d92bdd85147c33384f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e026a2f814384f8f97ea0af8bb08336c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80f125fab1d741a48145da4b27526938",
            "value": 0
          }
        },
        "80f125fab1d741a48145da4b27526938": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "858dd935d1f1497abefa3eabdfaeab3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95fc397687f24181a78d1ec05197a659": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c444e5edf88e4921a02c87dd07cf8899",
            "placeholder": "​",
            "style": "IPY_MODEL_dc6a99c262c24f3b83397ceb0dedcef8",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "9e48be389f3f494182ec13b7778c656a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc2d64590911437e8f0c37a63486b2ef",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49a1fedaa5c94cca9d86b4cf5fa2625d",
            "value": 203
          }
        },
        "bc2d64590911437e8f0c37a63486b2ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c444e5edf88e4921a02c87dd07cf8899": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5057f3071b443cead701f4e44a7da43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc6a99c262c24f3b83397ceb0dedcef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfb0661d4f7a4d439f2d00f0e825fb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e026a2f814384f8f97ea0af8bb08336c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e683fef138a04ba09489eec79fc93362": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5057f3071b443cead701f4e44a7da43",
            "placeholder": "​",
            "style": "IPY_MODEL_7040d05f895243808b9aba24110a5655",
            "value": "Map: 100%"
          }
        },
        "e98454d191654df892344c5121196210": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e683fef138a04ba09489eec79fc93362",
              "IPY_MODEL_9e48be389f3f494182ec13b7778c656a",
              "IPY_MODEL_0e300d5917ca47f6b0f51f96cb98f8ec"
            ],
            "layout": "IPY_MODEL_15aef94f2a4640c59e1f0fad3e75d206"
          }
        },
        "faf2a83f634a40b6a9355381f08e5a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58e84a7dd52940c7a87623dcfc6e2570",
            "placeholder": "​",
            "style": "IPY_MODEL_858dd935d1f1497abefa3eabdfaeab3a",
            "value": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
