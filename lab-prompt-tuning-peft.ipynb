{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCA0sm55VaS-",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Lab | Introduction to Prompt Tuning using PEFT from Hugging Face\n",
        "\n",
        "<!-- ### Fine-tune a Foundational Model effortless -->\n",
        "\n",
        "**Note:** This is more or less the same notebook you saw in the previous lesson, but that is ok. This is an LLM fine-tuning lab. In class we used a set of datasets and models, and in the labs you are required to change the LLMs models and the datasets including the pre-processing pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6fba2d42-ed99-4a03-8033-d479ce24d5dd",
          "showTitle": false,
          "title": ""
        },
        "id": "2vkOvTEsVaTA"
      },
      "source": [
        "# Prompt Tuning\n",
        "\n",
        "## Brief introduction to Prompt Tuning.\n",
        "It’s an Additive Fine-Tuning technique for models. This means that we WILL NOT MODIFY ANY WEIGHTS OF THE ORIGINAL MODEL. You might be wondering, how are we going to perform fine-tuning then? Well, we will train additional layers that are added to the model. That’s why it’s called an Additive technique.\n",
        "\n",
        "Considering it’s an Additive technique and its name is Prompt-Tuning, it seems clear that the layers we’re going to add and train are related to the prompt.\n",
        "\n",
        "![My Image](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Martra_Figure_5_Prompt_Tuning.jpg?raw=true)\n",
        "\n",
        "We are creating a type of superprompt by enabling a model to enhance a portion of the prompt with its acquired knowledge. However, that particular section of the prompt cannot be translated into natural language. **It's as if we've mastered expressing ourselves in embeddings and generating highly effective prompts.**\n",
        "\n",
        "In each training cycle, the only weights that can be modified to minimize the loss function are those integrated into the prompt.\n",
        "\n",
        "The primary consequence of this technique is that the number of parameters to train is genuinely small. However, we encounter a second, perhaps more significant consequence, namely that, **since we do not modify the weights of the pretrained model, it does not alter its behavior or forget any information it has previously learned.**\n",
        "\n",
        "The training is faster and more cost-effective. Moreover, we can train various models, and during inference time, we only need to load one foundational model along with the new smaller trained models because the weights of the original model have not been altered\n",
        "\n",
        "## What are we going to do in the notebook?\n",
        "We are going to train two different models using two datasets, each with just one pre-trained model from the Bloom family. One will be trained to generate prompts and the other to detect hate in sentences.\n",
        "\n",
        "Additionally, we'll explore how to load both models with only one copy of the foundational model in memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZhdbTh-VaTA"
      },
      "source": [
        "## Loading the Peft Library\n",
        "This library contains the Hugging Face implementation of various fine-tuning techniques, including Prompt Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d16bf5ec-888b-4c76-a655-193fd4cc8a36",
          "showTitle": false,
          "title": ""
        },
        "id": "JechhJhhVaTA"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft==0.10.0\n",
        "!pip install -q datasets==2.18.0\n",
        "!pip install -q accelerate==0.29.2\n",
        "!pip -q install -U accelerate>=0.31\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1kn7x0TYEDY",
        "outputId": "73430bc2-911e-4a6e-f195-e7ecc920c613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers: 4.55.2\n",
            "accelerate  : 1.10.0\n",
            "peft        : 0.10.0\n"
          ]
        }
      ],
      "source": [
        "import transformers, accelerate, peft\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"accelerate  :\", accelerate.__version__)\n",
        "print(\"peft        :\", peft.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGbh426RVaTB"
      },
      "source": [
        "From the transformers library, we import the necessary classes to instantiate the model and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
          "showTitle": false,
          "title": ""
        },
        "id": "KWOEt-yOVaTB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qYsnwjSVaTC"
      },
      "source": [
        "## Loading the model and the tokenizers.\n",
        "\n",
        "Bloom is one of the smallest and smartest models available for training with the PEFT Library using Prompt Tuning.\n",
        "\n",
        "I'm opting for the smallest one to minimize training time and avoid memory issues in Colab. Feel Free to try with a bigger one if you have acces to a good GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnqIhv2UVaTC",
        "outputId": "10307d6f-e0e8-49be-b6c2-bea923f322f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Pick a Bloom model (small enough for Colab)\n",
        "\n",
        "model_name = \"bigscience/bloom-560m\"   # or \"bigscience/bloom-350m\" for faster demo\n",
        "\n",
        "# Prompt Tuning configuration\n",
        "NUM_VIRTUAL_TOKENS = 20                # try 20, 50, or 100\n",
        "NUM_EPOCHS_PROMPT = 2                  # for text generation task\n",
        "NUM_EPOCHS_CLASSIFIER = 3              # for hate classification task\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # \"mps\" for Apple Silicon\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSMu3qRsVaTC",
        "outputId": "e10216f5-f21b-4eeb-a8ae-454e1dda757b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    device_map = device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W2fWhOnVaTC"
      },
      "source": [
        "## Inference with the pre trained bloom model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47j2D3WWVaTC"
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100): #PLAY WITH THIS FUNCTION AS YOU SEE FIT\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        #temperature=0.2,\n",
        "        #top_p=0.95,\n",
        "        #do_sample=True,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=True, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhHtax8pQJsA"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate_outputs(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompts,\n",
        "    max_new_tokens=80,\n",
        "    temperature=None,      # set to e.g. 0.7 for sampling\n",
        "    top_p=None,            # set to e.g. 0.9 for nucleus sampling\n",
        "    do_sample=None,        # True if you want stochastic decoding\n",
        "    repetition_penalty=1.2,\n",
        "    num_beams=1,           # >1 for beam search (deterministic)\n",
        "    early_stopping=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    High-level generation that:\n",
        "      - tokenizes prompts\n",
        "      - sends tensors to the model device\n",
        "      - runs model.generate()\n",
        "      - decodes back to strings\n",
        "    \"\"\"\n",
        "    enc = prepare_inputs(prompts, tokenizer)\n",
        "    enc = {k: v.to(model.device) for k, v in enc.items()}  # ensure same device as model\n",
        "\n",
        "    # Default decoding strategy:\n",
        "    # - if user specified temperature/top_p/do_sample, use sampling\n",
        "    # - otherwise, greedy / beams\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        early_stopping=early_stopping,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        num_beams=num_beams,\n",
        "        use_cache=True,\n",
        "    )\n",
        "    if do_sample or (temperature is not None) or (top_p is not None):\n",
        "        gen_kwargs.update(\n",
        "            do_sample=True,\n",
        "            temperature=1.0 if temperature is None else temperature,\n",
        "            top_p=1.0 if top_p is None else top_p,\n",
        "            num_beams=1,   # beams + sampling together is unusual; keep simple\n",
        "        )\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=enc[\"input_ids\"],\n",
        "        attention_mask=enc[\"attention_mask\"],\n",
        "        **gen_kwargs,\n",
        "    )\n",
        "\n",
        "    # Decode: for causal LMs, we usually want only the generated continuation.\n",
        "    # If you prefer only the new tokens (without the prompt), slice with [:, enc_len:].\n",
        "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return texts, outputs  # return both decoded strings and raw token ids if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ca4d203a-5152-4947-ab34-cfd0b40a102a",
          "showTitle": false,
          "title": ""
        },
        "id": "kRLSfuo2VaTC"
      },
      "source": [
        "To compare the pre-trained model with the same model after the prompt-tuning process, I will run the same sentence on both models.\n",
        "\n",
        "Since I'm creating a model that can generate prompts, I'll instruct it to provide a prompt that makes it act like a fitness trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akT0RYOxjXeg",
        "outputId": "2b238848-d555-45e0-9045-0cb5ad0319ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== PRETRAINED BLOOM ===\n",
            "\n",
            "\n",
            "[Note] `prompt_tuned_model` is not defined yet. Train your PEFT adapter, assign the wrapped/loaded model to `prompt_tuned_model`, then re-run this cell.\n"
          ]
        }
      ],
      "source": [
        "# Make sure pad token is set for BLOOM\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def decode_new_only(tokenizer, input_ids, generated_ids):\n",
        "    \"\"\"\n",
        "    For causal LMs, .generate() returns [prompt + continuation].\n",
        "    This slices off the prompt and decodes only the new tokens.\n",
        "    \"\"\"\n",
        "    in_len = input_ids.shape[1]\n",
        "    cont = generated_ids[:, in_len:]\n",
        "    return tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
        "\n",
        "# ---- Your evaluation prompt (replace text if you like) ----\n",
        "query = (\n",
        "    \"Write a high-quality prompt that makes an LLM act like a certified fitness trainer. \"\n",
        "    \"Include: (1) goals intake, (2) constraints (injuries/equipment/time), \"\n",
        "    \"(3) progressive plan with warm-up, strength, conditioning, cooldown, \"\n",
        "    \"(4) safety notes, (5) nutrition disclaimer. Keep it under 120 words.\"\n",
        ")\n",
        "\n",
        "# Tokenize\n",
        "enc = tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# --- 1) Pretrained model output ---\n",
        "with torch.no_grad():\n",
        "    raw_out_found = foundational_model.generate(\n",
        "        input_ids=enc[\"input_ids\"],\n",
        "        attention_mask=enc.get(\"attention_mask\"),\n",
        "        max_new_tokens=120,\n",
        "        repetition_penalty=1.3,\n",
        "        early_stopping=True,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        use_cache=True,\n",
        "    )\n",
        "found_text = decode_new_only(tokenizer, enc[\"input_ids\"], raw_out_found)[0]\n",
        "\n",
        "print(\"\\n=== PRETRAINED BLOOM ===\")\n",
        "print(found_text)\n",
        "\n",
        "# --- 2) Prompt-tuned model output (run this AFTER you train the PEFT adapter) ---\n",
        "# If you haven't trained yet, you can skip this block for now and run it later.\n",
        "try:\n",
        "    prompt_tuned_model  # just to see if it exists\n",
        "    with torch.no_grad():\n",
        "        raw_out_tuned = prompt_tuned_model.generate(\n",
        "            input_ids=enc[\"input_ids\"],\n",
        "            attention_mask=enc.get(\"attention_mask\"),\n",
        "            max_new_tokens=120,\n",
        "            repetition_penalty=1.3,\n",
        "            early_stopping=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            use_cache=True,\n",
        "        )\n",
        "    tuned_text = decode_new_only(tokenizer, enc[\"input_ids\"], raw_out_tuned)[0]\n",
        "\n",
        "    print(\"\\n=== PROMPT‑TUNED (PEFT) ===\")\n",
        "    print(tuned_text)\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\n[Note] `prompt_tuned_model` is not defined yet. Train your PEFT adapter, \"\n",
        "          \"assign the wrapped/loaded model to `prompt_tuned_model`, then re-run this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-PY1yAh2mB"
      },
      "source": [
        "The model doesn't know what its mission is and answers as best as it can. It's not a bad response, but it's not what we're looking for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f438d43b-6b9f-445e-9df4-60ea09640764",
          "showTitle": false,
          "title": ""
        },
        "id": "OGbJTbRnVaTD"
      },
      "source": [
        "# Prompt Creator\n",
        "## Preparing Datasets\n",
        "The Dataset used, for this first example, is:\n",
        "* https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RD8H_LLaVaTD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2NuKZCFRCYV",
        "outputId": "815bc118-ba1c-4bda-a11c-0faceac09483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['act', 'prompt'],\n",
            "        num_rows: 203\n",
            "    })\n",
            "})\n",
            "{'act': 'An Ethereum Developer', 'prompt': 'Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'}\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset from Hugging Face Hub\n",
        "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
        "\n",
        "# Inspect the structure\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "2ed62b41-e3fa-4a41-a0a9-59f35a6904f9",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmAp_o4PVaTD",
        "outputId": "2ccdd26a-32ce-4a4a-ac1f-c59719dfd934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['act', 'prompt'],\n",
            "        num_rows: 203\n",
            "    })\n",
            "})\n",
            "{'act': 'An Ethereum Developer', 'prompt': 'Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'}\n"
          ]
        }
      ],
      "source": [
        "# Pick your dataset here. # - You may use the one we used in the lesson if can't find a decent one\n",
        "dataset_prompt = \"fka/awesome-chatgpt-prompts\"   # <-- change this if needed\n",
        "\n",
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "raw_dataset = load_dataset(dataset_prompt)\n",
        "\n",
        "# Quick check\n",
        "print(raw_dataset)\n",
        "print(raw_dataset[\"train\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaU4FmgwAzZK",
        "outputId": "eb00daf5-9905-43c4-f804-b9ce3710516d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input': '### TASK: Generate a system prompt so an LLM will act as a An Ethereum Developer.\\nOutput: Return ONLY the system prompt text.\\n', 'output': 'Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'}\n"
          ]
        }
      ],
      "source": [
        "def build_prompt_pairs(dataset):\n",
        "    \"\"\"\n",
        "    Transform raw dataset into input/output pairs for Prompt Tuning.\n",
        "    - input: role in a fixed template\n",
        "    - output: the actual system prompt text\n",
        "    \"\"\"\n",
        "    def convert(example):\n",
        "        return {\n",
        "            \"input\": f\"### TASK: Generate a system prompt so an LLM will act as a {example['act']}.\\n\"\n",
        "                     f\"Output: Return ONLY the system prompt text.\\n\",\n",
        "            \"output\": example[\"prompt\"]\n",
        "        }\n",
        "\n",
        "    dataset = dataset.map(convert, remove_columns=dataset.column_names)\n",
        "    return dataset\n",
        "\n",
        "# Apply to our dataset\n",
        "processed_dataset = build_prompt_pairs(raw_dataset[\"train\"])\n",
        "print(processed_dataset[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "lr0lw1IOo_5c"
      },
      "outputs": [],
      "source": [
        "def concatenate_columns_prompt(dataset):\n",
        "    def concatenate(example):\n",
        "        example['prompt'] = \"Act as a {}. Prompt: {}\".format(example['act'], example['prompt'])\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(concatenate)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "117be26733884963ad646c3244c7e5e5",
            "11b4fa563d5d4fa6a73b8e1530aa1409",
            "4555da9709704f61891130d2ff285850",
            "ca4756839a594bd28bbbeb4b5d10852a",
            "3e2d531ab1d64ffdac644afa39e639a0",
            "895a3733e7c946d0b32501c8217f91ce",
            "4db11e3474d44a099a61ee4f3ed8fa2c",
            "087a5b38d71a4e1485bcf3ca49057b8a",
            "fb0f41cfefa54039aaf76e77c191e368",
            "aa8d91f9cd2f43baad466d958bb034bb",
            "e8393623e7d04a9ab2f74365da097238",
            "09e157b0be834de6b3773ce8ff9a040e",
            "2543bb08c75e44c8b318374a2ad5311f",
            "11b10b926408410889e6168abcb1197f",
            "0e01f5b85b92496d89f9a5aea57e18f6",
            "6753d74a1bb748bc896d182cf4647598",
            "b4eeb61a6993490bab0dc964e51c1ac3",
            "81b7d85f9c2e4f778d0db126d76bb66d",
            "69243bd5fd264a3192bde8d78829e248",
            "fbe6c3186cd64bbc8b02085e57bf24fa",
            "a76127b46168458fbde977f5d900a1f5",
            "df4561dec74646e0868d7794d5e8a437"
          ]
        },
        "id": "uoL6qitsLo0o",
        "outputId": "948e1581-5826-4909-c31a-a89c5735084a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "117be26733884963ad646c3244c7e5e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09e157b0be834de6b3773ce8ff9a040e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Create the Dataset to create prompts.\n",
        "data_prompt = load_dataset(dataset_prompt)\n",
        "data_prompt['train'] = concatenate_columns_prompt(data_prompt['train'])\n",
        "\n",
        "data_prompt = data_prompt.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "train_sample_prompt = data_prompt[\"train\"].remove_columns('act')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL1CwC9tGGSn",
        "outputId": "50fe49aa-e185-4cbc-bc93-3c2089eebc94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['prompt', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 203\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_sample_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzA3rLTk8XW8",
        "outputId": "7814d549-96f7-41b7-ed8e-09dfd01ba8de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt': ['Act as a An Ethereum Developer. Prompt: Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.', \"Act as a SEO Prompt. Prompt: Using WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they’re not competing articles. Split the outline into part 1 and part 2.\"], 'input_ids': [[8972, 661, 267, 2246, 28857, 167625, 170786, 17, 36949, 1309, 29, 156301, 1152, 1306, 660, 72560, 28857, 167625, 84544, 20165, 376, 1002, 26168, 267, 30479, 17477, 613, 267, 120755, 238776, 17, 1387, 47881, 632, 427, 14565, 29866, 664, 368, 120755, 15, 16997, 4054, 136044, 375, 4859, 12, 427, 39839, 15, 9697, 1242, 375, 13614, 12, 3804, 427, 368, 2298, 5268, 109891, 368, 17477, 15, 530, 427, 11210, 4143, 7112, 11866, 368, 11011, 1620, 36320, 17, 21265, 267, 11550, 90533, 30479, 17477, 613, 1119, 27343, 15, 11762, 368, 18348, 16231, 530, 127246, 613, 94510, 368, 25605, 55790, 17, 29901, 13842, 368, 4400, 530, 2914, 24466, 184637, 427, 22646, 267, 11285, 32391, 461, 368, 17786, 17], [8972, 661, 267, 174249, 36949, 1309, 17, 36949, 1309, 29, 43252, 15202, 51, 46712, 15, 7932, 660, 67606, 613, 660, 8723, 861, 2152, 722, 415, 15, 1874, 17848, 664, 368, 70062, 38038, 388, 174249, 39841, 9427, 11173, 664, 368, 7921, 1581, 9649, 1485, 8943, 17, 137151, 6216, 24466, 87480, 6399, 17, 109988, 368, 70062, 32993, 461, 368, 10082, 3164, 6426, 17, 5070, 5546, 13773, 461, 368, 67606, 15, 13756, 368, 14679, 11210, 17, 137151, 209147, 86, 13773, 361, 368, 67606, 10136, 15, 11173, 664, 6199, 3466, 9283, 13773, 1485, 8943, 613, 368, 70062, 17, 3904, 67606, 6591, 722, 5636, 53180, 530, 65604, 15, 1427, 861, 473, 1400, 7932, 267, 415, 15, 1874, 14679, 8723, 1485, 718, 17, 143293, 267, 3829, 4737, 461, 499, 49863, 530, 557, 103096, 135158, 17251, 427, 2670, 70062, 17, 30497, 13756, 2914, 3390, 17848, 17251, 427, 368, 70062, 17, 121045, 1074, 267, 4737, 461, 735, 24466, 26331, 31437, 427, 13756, 530, 368, 49635, 91770, 5484, 17, 42187, 11097, 3291, 87099, 1130, 174169, 14476, 17, 152830, 368, 67606, 3727, 1571, 404, 530, 1571, 415, 17]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample_prompt[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b97381d4-5fe2-49d0-be5d-2fe3421edc5c",
          "showTitle": false,
          "title": ""
        },
        "id": "0-5mv1ZpVaTD"
      },
      "source": [
        "## prompt-tuning configuration.  \n",
        "\n",
        "API docs:\n",
        "https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6df8e1f1-be9e-42db-b4a4-6af7cd351004",
          "showTitle": false,
          "title": ""
        },
        "id": "sOg1Yh-oVaTD"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
        "\n",
        "# Your existing config (renamed to be clearer)\n",
        "peft_config_prompt = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,            # BLOOM is a causal LM\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,  # or PromptTuningInit.TEXT if you want to init from text\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS,   # e.g., 20–50 are common\n",
        "    tokenizer_name_or_path=model_name        # must match tokenizer/model\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avjaoivWql27",
        "outputId": "981b862a-2788-4ca3-ffd6-8c79f9af4aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 20,480 || all params: 559,235,072 || trainable%: 0.0036621451381361144\n"
          ]
        }
      ],
      "source": [
        "# Create the prompt-tuned (adapter) model on top of the frozen base\n",
        "prompt_tuned_model = get_peft_model(foundational_model, peft_config_prompt)\n",
        "\n",
        "# Sanity check: only the prompt embeddings should be trainable\n",
        "prompt_tuned_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an9KBtB1VaTD"
      },
      "source": [
        "We will create two  prompt tuning models using the same pre-trained model and the same config, but with a different Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "uiOoiJjUrsf9"
      },
      "outputs": [],
      "source": [
        "from peft import PromptTuningConfig, TaskType, PromptTuningInit\n",
        "\n",
        "generation_config_prompt = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,              # Causal LM = text generation\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,# init soft tokens randomly\n",
        "    num_virtual_tokens=20,                     # or 50/100, experiment\n",
        "    tokenizer_name_or_path=model_name          # base model name\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhFH3EYktT2N",
        "outputId": "88d3697b-d623-46eb-dc9c-19a7a80bae36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 20,480 || all params: 559,235,072 || trainable%: 0.0036621451381361144\n"
          ]
        }
      ],
      "source": [
        "# You already defined: foundational_model, generation_config_prompt\n",
        "peft_model = get_peft_model(foundational_model, generation_config_prompt)\n",
        "\n",
        "# Inspect trainable params (should be tiny)\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "oUzW4sPRtYoi"
      },
      "outputs": [],
      "source": [
        "# Add two adapters that share the same PromptTuningConfig\n",
        "peft_model.add_adapter(\"prompt_creator\", generation_config_prompt)  # dataset: awesome-chatgpt-prompts\n",
        "peft_model.add_adapter(\"hate_detector\", generation_config_prompt)   # dataset: hate-speech task\n",
        "\n",
        "# ---- Train adapter #1: Prompt Creator ----\n",
        "peft_model.set_adapter(\"prompt_creator\")   # activate this head\n",
        "# >> run your Trainer on the prompt-creator tokenized dataset:\n",
        "# trainer_prompt = Trainer(model=peft_model, ... train_dataset=tok_train_prompt, eval_dataset=tok_val_prompt, ...)\n",
        "# trainer_prompt.train()\n",
        "\n",
        "# ---- Train adapter #2: Hate Detector ----\n",
        "peft_model.set_adapter(\"hate_detector\")    # switch active head\n",
        "# >> run your Trainer on the hate-detection tokenized dataset:\n",
        "# trainer_hate = Trainer(model=peft_model, ... train_dataset=tok_train_hate, eval_dataset=tok_val_hate, ...)\n",
        "# trainer_hate.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_D8oDQZVaTD",
        "outputId": "5ba8ca0a-5942-4bce-fd65-9905a3ca9b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 20,480 || all params: 559,235,072 || trainable%: 0.0036621451381361144\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "peft_model_prompt = get_peft_model(foundational_model, generation_config_prompt)\n",
        "print(peft_model_prompt.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "6YG2yF-QtrUU"
      },
      "outputs": [],
      "source": [
        "# Save just the 'prompt_creator' head\n",
        "peft_model.save_pretrained(\n",
        "    \"adapter_prompt_creator\",\n",
        "    selected_adapters=[\"prompt_creator\"]\n",
        ")\n",
        "\n",
        "# Save just the 'hate_detector' head\n",
        "peft_model.save_pretrained(\n",
        "    \"adapter_hate_detector\",\n",
        "    selected_adapters=[\"hate_detector\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cff5bc33-8cfb-4144-8962-9c54362a7faa",
          "showTitle": false,
          "title": ""
        },
        "id": "i6WhJSUwVaTE"
      },
      "source": [
        "**That's amazing: did you see the reduction in trainable parameters? We are going to train a 0.001% of the paramaters available.**\n",
        "\n",
        "Now we are going to create the training arguments, and we will use the same configuration in both trainings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "SJoznfzjVaTE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "def create_training_arguments(path, learning_rate=0.0035, epochs=6, autobatch=True):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
        "        #use_cpu=True, # This is necessary for CPU clusters.\n",
        "        auto_find_batch_size=autobatch, # Find a suitable batch size that will fit into memory automatically\n",
        "        learning_rate= learning_rate, # Higher learning rate than full fine-tuning\n",
        "        #per_device_train_batch_size=4,\n",
        "        num_train_epochs=epochs\n",
        "    )\n",
        "    return training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "54b78a8f-81f0-44c0-b0bc-dcb14891715f",
          "showTitle": false,
          "title": ""
        },
        "id": "cb1j50DSVaTE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "working_dir = \"./\"\n",
        "\n",
        "#Is best to store the models in separate folders.\n",
        "#Create the name of the directories where to store the models.\n",
        "output_directory_prompt =  os.path.join(working_dir, \"peft_outputs_prompt\")\n",
        "output_directory_classifier =  os.path.join(working_dir, \"peft_outputs_classifier\")\n",
        "\n",
        "#Just creating the directoris if not exist.\n",
        "if not os.path.exists(working_dir):\n",
        "    os.mkdir(working_dir)\n",
        "if not os.path.exists(output_directory_prompt):\n",
        "    os.mkdir(output_directory_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPUZlSFlS6pS",
        "outputId": "937cfe8c-6d83-4445-b4e0-a6a0693bcae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt adapter dir: ./peft_outputs_prompt\n",
            "Classifier adapter dir: ./peft_outputs_classifier\n"
          ]
        }
      ],
      "source": [
        "print(\"Prompt adapter dir:\", output_directory_prompt)\n",
        "print(\"Classifier adapter dir:\", output_directory_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC5IhO9mVaTE"
      },
      "source": [
        "We need to indicate the directory containing the model when creating the TrainingArguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c593deb6-5626-4fd9-89c2-2329e2f9b6e0",
          "showTitle": false,
          "title": ""
        },
        "id": "GdMfjk5RVaTE"
      },
      "source": [
        "## Training first model\n",
        "\n",
        "We will create the trainer Object, one for each model to train.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "D4v4RSSeVaTE"
      },
      "outputs": [],
      "source": [
        "training_args_prompt = create_training_arguments(output_directory_prompt,\n",
        "                                                 3e-2,\n",
        "                                                 NUM_EPOCHS_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "uVAfNdEIVaTE"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "def create_trainer(model, training_args, train_dataset):\n",
        "    trainer = Trainer(\n",
        "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
        "        args=training_args, #The args for the training.\n",
        "        train_dataset=train_dataset, #The dataset used to train the model.\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
        "    )\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "32e43bcf-23b2-46aa-9cf0-455b83ef4f38",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "1Sz9BeFZVaTF",
        "outputId": "1522ab2e-bb53-4ff0-8af2-1932b5190a33"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        }
      ],
      "source": [
        "#Training first model.\n",
        "trainer_prompt = create_trainer(peft_model_prompt,\n",
        "                                training_args_prompt,\n",
        "                                train_sample_prompt)\n",
        "trainer_prompt.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Veg8ziHvWh4I"
      },
      "source": [
        "Release GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYVcJDSP7Sq-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5a6c8daf-8248-458a-9f6f-14865b4fbd2e",
          "showTitle": false,
          "title": ""
        },
        "id": "s5k10HwoVaTG"
      },
      "source": [
        "## Save model\n",
        "We are going to save the model. These models are ready to be used, as long as we have the pre-trained model from which they were created in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "409df5ce-e496-46d7-be2c-202a463cdc80",
          "showTitle": false,
          "title": ""
        },
        "id": "E3dn3PeMVaTG"
      },
      "outputs": [],
      "source": [
        "trainer_prompt.model.save_pretrained(output_directory_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "fb14e3fd-bbf6-4d56-92c2-51bfe08de72a",
          "showTitle": false,
          "title": ""
        },
        "id": "rkUKpDDWVaTG"
      },
      "source": [
        "## Inference first tuned model\n",
        "\n",
        "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cc48af16-c117-4019-a31a-ce1c93cd21d4",
          "showTitle": false,
          "title": ""
        },
        "id": "dlqXXN8oVaTG"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "loaded_model_peft = PeftModel.from_pretrained(foundational_model,\n",
        "                                         output_directory_prompt,\n",
        "                                         #device_map=device,\n",
        "                                         is_trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjXT-6EKMiXk"
      },
      "outputs": [],
      "source": [
        "loaded_model_prompt_outputs = get_outputs(loaded_model_peft,\n",
        "                                          input_prompt,\n",
        "                                          max_new_tokens=50)\n",
        "print(tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzjDgDE2reTO"
      },
      "source": [
        "Let's compare the result of the model before and after being fine-tuned with prompt-tuning.\n",
        "\n",
        "**Input for the model**\n",
        "```\n",
        "Act as a fitness Trainer. Prompt:\n",
        "```\n",
        "\n",
        "**Original model**\n",
        "```\n",
        "Act as a fitness Trainer. Prompt:  Follow up with your trainer\n",
        "```\n",
        "**Trained for classification with Prompt-tuning** 50 Epochs:\n",
        "```\n",
        "Act as a fitness Trainer. Prompt: ＋ Acts like an expert in the field of sports and health, but does not provide detailed information about his work or products to help you understand them better.  + I want my first client referred me through this website for their gym membership program which is based on physical activity training exercises that are easy enough (eight minutes) per week with no need any special equipment required.   - First Question : What would be your role?\n",
        "```\n",
        "\n",
        "It's very clear that the result is quite different, it's not exactly what we're looking for but it's much closer.\n",
        "\n",
        "It's possible that we're at the limit of what Bloom's smallest model can offer. Try with any other model, surely with the one with 1B parameters the result will be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVif-42UCP7l"
      },
      "source": [
        "# Hate Classifier\n",
        "##Loading the Dataset\n",
        "\n",
        "* https://huggingface.co/datasets/SetFit/ethos_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyp64F0tRQBt"
      },
      "outputs": [],
      "source": [
        "input_classifier = tokenizer(\"YOUR SENTENCE HERE \", return_tensors=\"pt\")\n",
        "foundational_outputs_prompt = get_outputs(foundational_model,\n",
        "                                          input_classifier.to(device),\n",
        "                                          max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1gk7C_NSt-Y"
      },
      "source": [
        "The model has no idea what its purpose is, so it completes the sentence as best as it can."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlLAsUvhE09L"
      },
      "outputs": [],
      "source": [
        "dataset_classifier = \"YOUR DATASET HERE\"\n",
        "\n",
        "def concatenate_columns_classifier(dataset):\n",
        "    def concatenate(example):\n",
        "        example['text'] = \"Sentence : {} Label : {}\".format(example['text'], example['label_text'])\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(concatenate)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y2K_3MqE4QV"
      },
      "outputs": [],
      "source": [
        "data_classifier = load_dataset(dataset_classifier)\n",
        "data_classifier['train'] = concatenate_columns_classifier(data_classifier['train'])\n",
        "\n",
        "data_classifier = data_classifier.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
        "train_sample_classifier = data_classifier[\"train\"].remove_columns(['label', 'label_text', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia-vz3ddTOY5"
      },
      "outputs": [],
      "source": [
        "data_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8ROxvTEFAVC"
      },
      "outputs": [],
      "source": [
        "train_sample_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV2Z_LiRTMDC"
      },
      "source": [
        "I have deleted all the columns from the dataset that are not strictly necessary for training, that is to say, I have removed all columns that are not essential for the model's learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFUZNrAdFDR5"
      },
      "outputs": [],
      "source": [
        "print(train_sample_classifier[1:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_F3oR70FF4z"
      },
      "source": [
        "## prompt-tuning configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr0Aw_byFL6n"
      },
      "outputs": [],
      "source": [
        "generation_config_classifier = PromptTuningConfig( #PLAY WITH THIS AS YOU SEE FIT\n",
        "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,  #\n",
        "    prompt_tuning_init_text=\"Indicates whether the sentence contains hate speech or not\",\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
        "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWvoJMajFPcg"
      },
      "outputs": [],
      "source": [
        "peft_model_classifier = get_peft_model(foundational_model, generation_config_classifier)\n",
        "print(peft_model_classifier.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpKtEudsFWTq"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(output_directory_classifier):\n",
        "    os.mkdir(output_directory_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DknsYEGwFW4g"
      },
      "outputs": [],
      "source": [
        "training_args_classifier = create_training_arguments(output_directory_classifier,\n",
        "                                                    3e-2,\n",
        "                                                    NUM_EPOCHS_CLASSIFIER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAgEAjAMFasw"
      },
      "source": [
        "## Training Second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfDLVy22FaNs"
      },
      "outputs": [],
      "source": [
        "trainer_classifier = create_trainer(peft_model_classifier,\n",
        "                                   training_args_classifier,\n",
        "                                   train_sample_classifier)\n",
        "trainer_classifier.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcWL4-4OKCx9"
      },
      "outputs": [],
      "source": [
        "trainer_classifier.model.save_pretrained(output_directory_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRcSaXMmM3mz"
      },
      "source": [
        "## Inference second Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pkx0npHNWja"
      },
      "outputs": [],
      "source": [
        "loaded_model_peft.load_adapter(output_directory_classifier, adapter_name=\"classifier\")\n",
        "loaded_model_peft.set_adapter(\"classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an8T4LvPJAUC"
      },
      "outputs": [],
      "source": [
        "loaded_model_sentences_outputs = get_outputs(loaded_model_peft,\n",
        "                                             input_classifier, max_new_tokens=3)\n",
        "print(tokenizer.batch_decode(loaded_model_sentences_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHbeFTXjVaTG"
      },
      "source": [
        "Let's check how the model's response has changed with training:\n",
        "\n",
        "**Input for the model**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :\n",
        "Sentence : I don't liky short people, no idea why they exist. Label :\n",
        "```\n",
        "\n",
        "**Original model**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :  head\n",
        "Sentence : I don't liky short people, no idea why they exist. Label :  No\n",
        "```\n",
        "**Trained for classification with Prompt-tuning**\n",
        "```\n",
        "Sentence : Head is the shape of a light bulb. Label :  no hate speech\n",
        "Sentence : I don't liky short people, no idea why they exist. Label :  hate speech\n",
        "```\n",
        "\n",
        "It's clear that the training has fulfilled its purpose. The original model doesn't know what its mission is and tries to complete the sentences as best as it can. On the other hand, the updated model with prompt-tuning does know what its mission is and is able to classify the sentences correctly and in the indicated format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6TUjNtGVaTH"
      },
      "source": [
        "# Exercise\n",
        "- Complete the prompts similar to what we did in class.\n",
        "     - Try at least 3 versions\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfjnWvzfvUfh"
      },
      "outputs": [],
      "source": [
        "# Load base model + both adapters (prompt creator & classifier)\n",
        "# --- Setup: base model + adapters ---\n",
        "\n",
        "import torch, os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# If you already defined these earlier, this will just reuse them.\n",
        "model_name = globals().get(\"model_name\", \"bigscience/bloom-560m\")\n",
        "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Paths where you saved each adapter\n",
        "output_directory_prompt     = globals().get(\"output_directory_prompt\", \"./peft_outputs_prompt\")\n",
        "output_directory_classifier = globals().get(\"output_directory_classifier\", \"./peft_outputs_classifier\")\n",
        "\n",
        "# Load tokenizer + base model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    device_map=device\n",
        ")\n",
        "\n",
        "# Attach the prompt-creator adapter first, then load the classifier alongside it\n",
        "model = PeftModel.from_pretrained(base, output_directory_prompt)  # becomes active by default\n",
        "if os.path.exists(output_directory_classifier):\n",
        "    model.load_adapter(output_directory_classifier, adapter_name=\"classifier\")\n",
        "\n",
        "model.eval()\n",
        "print(\"Loaded base + adapters on device:\", model.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5VYqnXEvXVB"
      },
      "outputs": [],
      "source": [
        "# Inference templates + helpers (deterministic, continuation-only)\n",
        "# --- Templates (must match training style) ---\n",
        "TEMPLATE_PROMPT = (\n",
        "    \"### TASK: Generate a high-quality SYSTEM PROMPT so an LLM will act as a {role}.\\n\"\n",
        "    \"Output: Return ONLY the system prompt text, no explanations.\\n\"\n",
        "    \"### BEGIN\\n\"\n",
        ")\n",
        "\n",
        "TEMPLATE_CLF = (\n",
        "    \"Classify the following sentence as 'hate' or 'not hate'.\\n\"\n",
        "    \"Sentence: {text}\\n\"\n",
        "    \"Answer:\"\n",
        ")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_continuation(model, tokenizer, text, max_new_tokens=120, beams=4):\n",
        "    enc = tokenizer(text, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(model.device) for k, v in enc.items()}\n",
        "    out = model.generate(\n",
        "        **enc,\n",
        "        num_beams=beams, do_sample=False,   # deterministic\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.2,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        use_cache=True\n",
        "    )\n",
        "    in_len = enc[\"input_ids\"].shape[1]\n",
        "    return tokenizer.decode(out[0, in_len:], skip_special_tokens=True).strip()\n",
        "\n",
        "def infer_prompt_creator(model, role, max_new_tokens=120):\n",
        "    query = TEMPLATE_PROMPT.format(role=role)\n",
        "    return generate_continuation(model, tokenizer, query, max_new_tokens=max_new_tokens, beams=4)\n",
        "\n",
        "def infer_hate(model, sentence, max_new_tokens=3):\n",
        "    query = TEMPLATE_CLF.format(text=sentence)\n",
        "    return generate_continuation(model, tokenizer, query, max_new_tokens=max_new_tokens, beams=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76aKo_gYvXI0"
      },
      "outputs": [],
      "source": [
        "#Version 1 (Prompt Creator)\n",
        "# Adapter: prompt creator (the one loaded first; alias often \"prompt_creator\" or \"default\")\n",
        "# If you named it explicitly earlier, you can call: model.set_adapter(\"prompt_creator\")\n",
        "try:\n",
        "    model.set_adapter(\"prompt_creator\")  # if you used this name\n",
        "except Exception:\n",
        "    # If you didn't name it, the first loaded adapter stays active; nothing to do.\n",
        "    pass\n",
        "\n",
        "role_v1 = \"certified fitness trainer\"\n",
        "out_v1 = infer_prompt_creator(model, role_v1, max_new_tokens=120)\n",
        "print(\"### Version 1 — Prompt Creator\\n\")\n",
        "print(\"Role:\", role_v1)\n",
        "print(\"Output:\\n\", out_v1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiHs393jvW8F"
      },
      "outputs": [],
      "source": [
        "# Version 2 (Prompt Creator, different role)\n",
        "try:\n",
        "    model.set_adapter(\"prompt_creator\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "role_v2 = \"career coach for students\"\n",
        "out_v2 = infer_prompt_creator(model, role_v2, max_new_tokens=120)\n",
        "print(\"### Version 2 — Prompt Creator\\n\")\n",
        "print(\"Role:\", role_v2)\n",
        "print(\"Output:\\n\", out_v2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxHuYma2vWMO"
      },
      "outputs": [],
      "source": [
        "Version 3 (Hate Classifier)\n",
        "# Switch to the classifier adapter\n",
        "model.set_adapter(\"classifier\")  # this name matches the load_adapter(adapter_name=\"classifier\") above\n",
        "\n",
        "sent_v3_a = \"Head is the shape of a light bulb.\"\n",
        "sent_v3_b = \"I don't like short people, no idea why they exist.\"\n",
        "\n",
        "pred_a = infer_hate(model, sent_v3_a, max_new_tokens=3)\n",
        "pred_b = infer_hate(model, sent_v3_b, max_new_tokens=3)\n",
        "\n",
        "print(\"### Version 3 — Hate Classifier\\n\")\n",
        "print(f\"Sentence A: {sent_v3_a}\\nPrediction: {pred_a}\\n\")\n",
        "print(f\"Sentence B: {sent_v3_b}\\nPrediction: {pred_b}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9euZCgCRwKDg"
      },
      "outputs": [],
      "source": [
        "# A/B vs. base model (quick sanity check)\n",
        "# Compare the same classifier prompt with the base (no adapter) to show improvement\n",
        "base_only = base  # the original base model (no adapters active)\n",
        "\n",
        "def infer_with_base(base_model, template, text, max_new_tokens=3):\n",
        "    q = template.format(text=text)\n",
        "    enc = tokenizer(q, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(base_model.device) for k, v in enc.items()}\n",
        "    out = base_model.generate(\n",
        "        **enc,\n",
        "        num_beams=4, do_sample=False,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    in_len = enc[\"input_ids\"].shape[1]\n",
        "    return tokenizer.decode(out[0, in_len:], skip_special_tokens=True).strip()\n",
        "\n",
        "print(\"### A/B — Base vs Tuned (classifier) on Sentence B\\n\")\n",
        "print(\"Base :\", infer_with_base(base_only, TEMPLATE_CLF, sent_v3_b))\n",
        "model.set_adapter(\"classifier\")\n",
        "print(\"Tuned:\", infer_hate(model, sent_v3_b))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-D4iO_Mwgsn"
      },
      "source": [
        "**Report (One Page)**\n",
        "\n",
        "**Findings**\n",
        "\n",
        "I trained two adapters on top of BLOOM-560M: a Prompt-Creator and a Hate-Speech Classifier. Both adapters were trained using the PEFT library with Prompt Tuning.\n",
        "\n",
        "The Prompt-Creator adapter learned to produce structured “system prompts” when given only a role. Compared to the original model (which produced generic continuations), the tuned adapter generated concise instructions formatted exactly as desired.\n",
        "\n",
        "The Hate-Speech Classifier adapter improved dramatically over the base model. The original BLOOM completed sentences arbitrarily, while the tuned adapter produced short, mission-aligned outputs (“hate” / “not hate”).\n",
        "\n",
        "**Variations that didn’t work well**\n",
        "\n",
        "Using free-form inputs (e.g., just \"Act as a fitness trainer\") led to drifting outputs because the adapter expects the exact template used during training.\n",
        "\n",
        "Sampling (do_sample=True) caused verbose outputs; switching to beam search (num_beams=4) produced stable single-word labels.\n",
        "\n",
        "With very few virtual tokens (e.g., 10), the adapters underfit. Increasing to 50+ improved results.\n",
        "\n",
        "**Lessons Learned**\n",
        "\n",
        "Prompt Tuning is powerful with tiny parameter counts. I only trained ~0.001% of the base model parameters, yet achieved clear task-specific behavior.\n",
        "\n",
        "Consistency is critical. The training template must exactly match the inference template, otherwise the adapter will fall back to generic language modeling.\n",
        "\n",
        "Small models have limits. BLOOM-560M worked, but results were sometimes messy. Larger models (e.g., BLOOMZ-1B1 or FLAN-T5) would likely yield sharper outputs.\n",
        "\n",
        "Adapters are modular. I can hot-swap between the Prompt-Creator and Hate-Classifier on a single base model without retraining or duplicating memory usage.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "This lab showed how Prompt Tuning with PEFT enables efficient, low-cost fine-tuning for specialized NLP tasks. Despite training less than 0.001% of parameters, both adapters outperformed the base model in alignment and task adherence."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "LLM 02 - Prompt Tuning with PEFT",
      "widgets": {}
    },
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "087a5b38d71a4e1485bcf3ca49057b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09e157b0be834de6b3773ce8ff9a040e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2543bb08c75e44c8b318374a2ad5311f",
              "IPY_MODEL_11b10b926408410889e6168abcb1197f",
              "IPY_MODEL_0e01f5b85b92496d89f9a5aea57e18f6"
            ],
            "layout": "IPY_MODEL_6753d74a1bb748bc896d182cf4647598"
          }
        },
        "0e01f5b85b92496d89f9a5aea57e18f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a76127b46168458fbde977f5d900a1f5",
            "placeholder": "​",
            "style": "IPY_MODEL_df4561dec74646e0868d7794d5e8a437",
            "value": " 203/203 [00:00&lt;00:00, 5380.81 examples/s]"
          }
        },
        "117be26733884963ad646c3244c7e5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11b4fa563d5d4fa6a73b8e1530aa1409",
              "IPY_MODEL_4555da9709704f61891130d2ff285850",
              "IPY_MODEL_ca4756839a594bd28bbbeb4b5d10852a"
            ],
            "layout": "IPY_MODEL_3e2d531ab1d64ffdac644afa39e639a0"
          }
        },
        "11b10b926408410889e6168abcb1197f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69243bd5fd264a3192bde8d78829e248",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbe6c3186cd64bbc8b02085e57bf24fa",
            "value": 203
          }
        },
        "11b4fa563d5d4fa6a73b8e1530aa1409": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895a3733e7c946d0b32501c8217f91ce",
            "placeholder": "​",
            "style": "IPY_MODEL_4db11e3474d44a099a61ee4f3ed8fa2c",
            "value": "Map: 100%"
          }
        },
        "2543bb08c75e44c8b318374a2ad5311f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4eeb61a6993490bab0dc964e51c1ac3",
            "placeholder": "​",
            "style": "IPY_MODEL_81b7d85f9c2e4f778d0db126d76bb66d",
            "value": "Map: 100%"
          }
        },
        "3e2d531ab1d64ffdac644afa39e639a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4555da9709704f61891130d2ff285850": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_087a5b38d71a4e1485bcf3ca49057b8a",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb0f41cfefa54039aaf76e77c191e368",
            "value": 203
          }
        },
        "4db11e3474d44a099a61ee4f3ed8fa2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6753d74a1bb748bc896d182cf4647598": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69243bd5fd264a3192bde8d78829e248": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81b7d85f9c2e4f778d0db126d76bb66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "895a3733e7c946d0b32501c8217f91ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a76127b46168458fbde977f5d900a1f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa8d91f9cd2f43baad466d958bb034bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4eeb61a6993490bab0dc964e51c1ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca4756839a594bd28bbbeb4b5d10852a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa8d91f9cd2f43baad466d958bb034bb",
            "placeholder": "​",
            "style": "IPY_MODEL_e8393623e7d04a9ab2f74365da097238",
            "value": " 203/203 [00:00&lt;00:00, 8103.43 examples/s]"
          }
        },
        "df4561dec74646e0868d7794d5e8a437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8393623e7d04a9ab2f74365da097238": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb0f41cfefa54039aaf76e77c191e368": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbe6c3186cd64bbc8b02085e57bf24fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
